---
layout: post
title: "Quantization in LLMs: From Fundamentals to Recent Trends"
---

<h5>
    Yoonjun Cho - AI-ISL, CS, Yonsei University
</h5>

Quantization reduces the bitwidth of weights in large language models to enable efficient inference, but often comes at the cost of accuracy. In this talk, I begin with an overview of quantization techniques, outlining their evolution and recent research trends aimed at improving quality and scalability.
I will then present my recent work on quantization, which focuses on decomposing the weight matrix into a quantized matrix and low-rank components. The talk concludes with a brief discussion of future directions in quantization research.

[PPT](https://drive.google.com/file/d/1kANUhMbtxmFsYG7GXdBnwnAEmOmpdwQi/view?usp=share_link)
[CV](https://cyoonjun.github.io/)

<i>
    Catering Courtesy of <a href="https://albert-no.github.io/">AI-ISL</a>
</i>
