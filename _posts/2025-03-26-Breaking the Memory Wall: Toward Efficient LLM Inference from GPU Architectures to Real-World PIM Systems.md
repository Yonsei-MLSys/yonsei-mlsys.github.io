---
layout: post
title: "Breaking the Memory Wall: Toward Efficient LLM Inference from GPU Architectures to Real-World PIM Systems"
---

<h5>
    Hyeoncheol Kim - ASO Lab, CS, Yonsei University
</h5>

As the data demands of large language models (LLMs) continue to grow, challenges such as the memory wall have become increasingly prominent in modern AI accelerators. These limitations lead to constrained scalability, underutilized compute resources, and increased energy consumption. This talk presents research conducted by Hyeoncheol Kim, which explores system-level techniques to improve the efficiency of LLM inference across both GPU-based systems and real-world Processing-In-Memory (PIM) hardware. The presentation covers architectural considerations and compiler-level integration strategies designed to reduce data movement overhead and enhance execution throughput.

[PPT](https://drive.google.com/file/d/1Z2XLVHEKWNNyF7bpp-8u6jXSsZMWRWGJ/view?usp=share_link)
[CV](https://www.linkedin.com/in/hyeoncheol-kim-8a2337285)

<i>
    Catering Courtesy of <a href="https://www.ciplab.kr/">CIP Lab</a>
</i>
