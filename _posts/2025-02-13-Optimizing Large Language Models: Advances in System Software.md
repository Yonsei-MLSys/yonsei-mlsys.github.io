---
layout: post
title: "Optimizing Large Language Models: Advances in System Software"
---

<h5>
    Heelim Choi - Core Lab, EE, Yonsei Univiersity
</h5>

Large Language Models (LLMs) have revolutionized AI applications, yet their high computational and memory demands pose significant challenges in inference and deployment.
This talk explores system software optimizations that enhance the efficiency of LLM execution.
We discuss key innovations such as PagedAttention (vLLM) for memory-efficient inference, GPU-accelerated serving with TensorRT-LLM, and distributed execution strategies in DeepSpeed and Megatron-LM.
These approaches enable higher throughput, lower latency, and reduced hardware costs, making LLM deployment more scalable.
Finally, we examine the real-world impact of these techniques and future directions in LLM system optimization.

[PPT](https://drive.google.com/file/d/1JV605D35TSxzAVYLJG0ONLL7pLUgJrVJ/view?usp=share_link)
[CV](https://sites.google.com/yonsei.ac.kr/heelim/profile)

<h6>
    <i>Catering Courtesy of ASO Lab</i>
</h6>
