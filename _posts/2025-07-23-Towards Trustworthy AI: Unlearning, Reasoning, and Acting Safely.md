---
layout: post
title: "Towards Trustworthy AI: Unlearning, Reasoning, and Acting Safely"
---

<h5>
    Wonje Jeung - AI-ISL, AI, Yonsei University
</h5>

As foundation models are increasingly integrated into real-world systems, ensuring privacy and safety across what they remember, how they reason, and how they act has become a pressing challenge. This talk introduces a set of complementary approaches addressing these concerns at multiple levels.
One direction explores how unlearning methods can inadvertently remove shared or non-sensitive knowledge, leading to unnecessary utility loss. This motivates a new perspective on unlearning fidelity in multi-source settings. Another thread focuses on the reasoning process of language models, where harm can emerge not just in outputs but during intermediate steps; aligning this process early enables models to stay on safer trajectories without sacrificing usefulness. In the context of embodied agents, an analysis of vision-language-action (VLA) models reveals that they often succeed at tasks without genuine risk awareness—highlighting the need for benchmarks that go beyond task success to evaluate safety-critical behavior.
Taken together, these efforts point toward more trustworthy AI systems—capable of forgetting precisely, reasoning safely, and acting with awareness.

[PPT](https://docs.google.com/presentation/d/1rxBz20vNd5Cp27mNiUxchx1sy7hJc28r/edit?usp=sharing&ouid=111948851444227468135&rtpof=true&sd=true)
[CV](https://drive.google.com/file/d/1nv4hYaR9thk7ABIOq4CDKVIqk31t1qdM/view?usp=sharing)

<i>
    Catering Courtesy of <a href="http://corelab.or.kr/index.php">CoreLab</a>
</i>
